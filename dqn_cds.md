{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Super Mario Bros 강화학습 튜토리얼\n",
    "\n",
    "이 튜토리얼에서는 `Deep Q-Networks (DQN)` 알고리즘을 사용하여 `Super Mario Bros` 게임을 플레이하는 에이전트를 구축하고 학습시킵니다. 우리는 `OpenAI Gym`, `gym-super-mario-bros`, `PyTorch`와 같은 라이브러리를 사용할 것입니다.\n",
    "\n",
    "### 1. 라이브러리 임포트\n",
    "\n",
    "먼저 필요한 라이브러리를 임포트합니다.\n",
    "\n",
    "```python\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym.wrappers import RecordEpisodeStatistics\n",
    "from gym.spaces import Box\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import cv2\n",
    "from collections import deque\n",
    "```\n",
    "\n",
    "- **IPython.display.clear_output**: 학습 중간에 출력 내용을 지우는 함수입니다.\n",
    "- **matplotlib.pyplot**: 학습 과정을 시각화하는 데 사용됩니다.\n",
    "- **numpy**: 수치 계산에 사용됩니다.\n",
    "- **gym**: 강화 학습 환경을 제공하는 라이브러리입니다.\n",
    "- **gym_super_mario_bros**: Super Mario Bros 환경을 제공하는 라이브러리입니다.\n",
    "- **torch** 및 **torch.nn**: PyTorch를 사용하여 신경망을 구축하고 학습시킵니다.\n",
    "- **random**: 랜덤한 행동 선택에 사용됩니다.\n",
    "- **cv2**: OpenCV 라이브러리로, 이미지 처리에 사용됩니다.\n",
    "- **collections.deque**: 경험을 저장하는 데 사용됩니다.\n",
    "\n",
    "### 2. 플로팅 함수\n",
    "\n",
    "학습 과정에서 보상과 손실을 시각화하기 위한 함수입니다.\n",
    "\n",
    "```python\n",
    "def plot(frame_idx, rewards, losses):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('frame %s. reward: %s' % (frame_idx, np.mean(rewards[-10:])))\n",
    "    plt.plot(rewards)\n",
    "    plt.subplot(132)\n",
    "    plt.title('loss')\n",
    "    plt.plot(losses)\n",
    "    plt.show()\n",
    "```\n",
    "\n",
    "- **plot** 함수: 학습 중간에 보상과 손실을 시각화하여 학습이 잘 되고 있는지 확인할 수 있습니다.\n",
    "- **clear_output(True)**: IPython의 출력을 지우는 함수입니다.\n",
    "- **plt.figure(figsize=(20,5))**: 그래프의 크기를 설정합니다.\n",
    "- **plt.subplot(131)**: 그래프를 나누어 그리기 위해 subplot을 설정합니다.\n",
    "- **plt.title**: 그래프의 제목을 설정합니다.\n",
    "- **plt.plot**: 그래프를 그립니다.\n",
    "- **TensorBoard 라는 것도 있음\n",
    "\n",
    "### 3. 프레임 전처리\n",
    "\n",
    "게임 화면을 흑백으로 변환하고 크기를 조정하는 래퍼 클래스입니다.\n",
    "\n",
    "```python\n",
    "class WarpFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "        self.width = 84\n",
    "        self.height = 84\n",
    "        self.observation_space = Box(low=0, high=255, shape=(self.height, self.width, 1), dtype=np.uint8)\n",
    "\n",
    "    def observation(self, frame):\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        frame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_AREA)\n",
    "        frame = frame[:, :, np.newaxis]  # Add a channel dimension\n",
    "        return frame\n",
    "```\n",
    "\n",
    "- **WarpFrame 클래스**: 게임 화면을 전처리하는 역할을 합니다.\n",
    "  - **__init__**: 프레임의 크기를 84x84로 설정합니다.\n",
    "  - **observation**: 프레임을 흑백으로 변환하고 크기를 조정합니다.\n",
    "    - **cv2.cvtColor**: 프레임을 흑백으로 변환합니다.\n",
    "    - **cv2.resize**: 프레임의 크기를 84x84로 조정합니다.\n",
    "\n",
    "### 4. 모델 정의\n",
    "\n",
    "신경망 모델을 정의하는 클래스입니다. `CnnDQN` 클래스는 입력 이미지를 받아서 각 행동의 Q-값을 출력합니다.\n",
    "\n",
    "```python\n",
    "class CnnDQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(CnnDQN, self).__init__()\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.feature_size(), 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, self.num_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def feature_size(self):\n",
    "        return self.features(torch.zeros(1, *self.input_shape)).view(1, -1).size(1)\n",
    "    \n",
    "    def act(self, state, epsilon):\n",
    "        if random.random() > epsilon:\n",
    "            with torch.no_grad():\n",
    "                state = torch.FloatTensor(np.float32(state)).unsqueeze(0)\n",
    "                q_value = self.forward(state)\n",
    "                action = q_value.max(1)[1].item()\n",
    "        else:\n",
    "            action = random.randrange(env.action_space.n)\n",
    "        return action\n",
    "```\n",
    "\n",
    "- **CnnDQN 클래스**: DQN 모델을 정의합니다.\n",
    "  - **__init__**: 모델의 레이어를 정의합니다.\n",
    "    - **self.features**: CNN 레이어를 정의합니다.\n",
    "      - **nn.Conv2d**: 컨볼루션 레이어를 추가합니다.\n",
    "      - **nn.ReLU**: 활성화 함수로 ReLU를 사용합니다.\n",
    "    - **self.fc**: 완전 연결 레이어를 정의합니다.\n",
    "      - **nn.Linear**: 완전 연결 레이어를 추가합니다.\n",
    "  - **forward**: 입력 데이터를 받아 forward pass를 수행합니다.\n",
    "  - **feature_size**: 입력 데이터의 크기를 계산합니다.\n",
    "  - **act**: 에이전트의 행동을 결정합니다.\n",
    "    - **torch.no_grad()**: 학습이 아닌 추론 모드로 전환합니다.\n",
    "    - **random.random() > epsilon**: 탐험과 활용을 결정합니다.\n",
    "\n",
    "### 5. Replay Buffer\n",
    "\n",
    "에이전트의 경험을 저장하고, 학습 시 샘플링하는 버퍼입니다.\n",
    "\n",
    "```python\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.array(state), action, reward, np.array(next_state), done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "```\n",
    "\n",
    "- **ReplayBuffer 클래스**: 경험을 저장하고 샘플링하는 버퍼입니다.\n",
    "  - **__init__**: 버퍼의 최대 크기를 설정합니다.\n",
    "  - **push**: 경험을 버퍼에 추가합니다.\n",
    "  - **sample**: 미니배치를 샘플링합니다.\n",
    "  - **__len__**: 버퍼의 현재 크기를 반환합니다.\n",
    "\n",
    "### 6. 하이퍼파라미터 및 유틸리티 함수\n",
    "\n",
    "학습에 필요한 하이퍼파라미터와 유틸리티 함수를 정의합니다.\n",
    "\n",
    "```python\n",
    "epsilon_start = 1.0\n",
    "epsilon_final = 0.01\n",
    "epsilon_decay = 30000\n",
    "epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * np.exp(-1. * frame_idx / epsilon_decay)\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "Variable = lambda *args, **kwargs: autograd.Variable(*args, **kwargs).cuda() if USE_CUDA else autograd.Variable(*args, **kwargs)\n",
    "```\n",
    "\n",
    "- **epsilon_start, epsilon_final, epsilon_decay**: 탐험과 활용 사이의 균형을 조절하는 파라미터입니다.\n",
    "- **epsilon_by_frame**: 현재 프레임 인덱스에 따라 epsilon 값을 계산합니다.\n",
    "- **USE_CUDA**: CUDA 사용 가능 여부를 확인합니다.\n",
    "- **Variable**: CUDA를 사용할 경우 torch.autograd.Variable을 CUDA 변수로 변환합니다.\n",
    "\n",
    "### 7. 환경 설정\n",
    "\n",
    "게임 환경을 설정하고 래퍼를 적용합니다.\n",
    "\n",
    "```python\n",
    "env = gym.make('SuperMarioBros-v0', apply_api_compatibility=True, render_mode=\"human\")\n",
    "env = JoypadSpace\n",
    "\n",
    "(env, SIMPLE_MOVEMENT)\n",
    "env = WarpFrame(env)\n",
    "```\n",
    "\n",
    "- **gym.make**: Super Mario Bros 환경을 생성합니다.\n",
    "- **JoypadSpace**: 간단한 조작만을 사용하도록 설정합니다.\n",
    "- **WarpFrame**: 프레임을 전처리합니다.\n",
    "\n",
    "### 8. 모델 및 옵티마이저 설정\n",
    "\n",
    "신경망 모델과 옵티마이저를 설정합니다.\n",
    "\n",
    "```python\n",
    "model = CnnDQN(env.observation_space.shape, env.action_space.n)\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "```\n",
    "\n",
    "- **CnnDQN**: DQN 모델을 초기화합니다.\n",
    "- **USE_CUDA**: CUDA가 사용 가능한 경우 모델을 CUDA로 이동합니다.\n",
    "- **optim.Adam**: Adam 옵티마이저를 설정합니다.\n",
    "\n",
    "### 9. 손실 계산 함수\n",
    "\n",
    "미니배치를 사용하여 손실을 계산하는 함수입니다.\n",
    "\n",
    "```python\n",
    "def compute_td_loss(batch_size):\n",
    "    state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "\n",
    "    state = Variable(torch.FloatTensor(np.float32(state)))\n",
    "    next_state = Variable(torch.FloatTensor(np.float32(next_state)))\n",
    "    action = Variable(torch.LongTensor(action))\n",
    "    reward = Variable(torch.FloatTensor(reward))\n",
    "    done = Variable(torch.FloatTensor(done))\n",
    "\n",
    "    q_values = model(state)\n",
    "    next_q_values = model(next_state)\n",
    "\n",
    "    q_value = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "    next_q_value = next_q_values.max(1)[0]\n",
    "    expected_q_value = reward + gamma * next_q_value * (1 - done)\n",
    "    \n",
    "    loss = (q_value - Variable(expected_q_value.data)).pow(2).mean()\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss\n",
    "```\n",
    "\n",
    "- **compute_td_loss**: 미니배치를 사용하여 손실을 계산합니다.\n",
    "  - **replay_buffer.sample**: 미니배치를 샘플링합니다.\n",
    "  - **Variable**: 텐서를 PyTorch 변수로 변환합니다.\n",
    "  - **model(state)**: 현재 상태에서 Q-값을 예측합니다.\n",
    "  - **q_value.gather**: 선택된 행동에 대한 Q-값을 가져옵니다.\n",
    "  - **expected_q_value**: 기대되는 Q-값을 계산합니다.\n",
    "  - **loss.backward()**: 역전파를 통해 그래디언트를 계산합니다.\n",
    "  - **optimizer.step()**: 옵티마이저를 통해 모델 파라미터를 업데이트합니다.\n",
    "\n",
    "### 10. 학습 루프\n",
    "\n",
    "에이전트를 학습시키는 메인 루프입니다.\n",
    "\n",
    "```python\n",
    "num_frames = 300000\n",
    "batch_size = 32\n",
    "gamma = 0.99\n",
    "\n",
    "losses = []\n",
    "all_rewards = []\n",
    "episode_reward = 0\n",
    "\n",
    "state = env.reset()\n",
    "if isinstance(state, tuple):\n",
    "    state = state[0]\n",
    "print(\"Initial state shape:\", state.shape)  # Print the initial state shape\n",
    "for frame_idx in range(1, num_frames + 1):\n",
    "    epsilon = epsilon_by_frame(frame_idx)\n",
    "    action = model.act(state, epsilon)\n",
    "    \n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    if isinstance(obs, tuple):\n",
    "        obs = obs[0]\n",
    "    replay_buffer.push(state, action, reward, obs, done)\n",
    "    \n",
    "    state = obs\n",
    "    episode_reward += reward\n",
    "    \n",
    "    if done:\n",
    "        state = env.reset()\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]\n",
    "        print(\"New episode initial state shape:\", state.shape)  # Print the state shape at the beginning of each episode\n",
    "        all_rewards.append(episode_reward)\n",
    "        episode_reward = 0\n",
    "        \n",
    "    if len(replay_buffer) > replay_initial:\n",
    "        loss = compute_td_loss(batch_size)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "    if frame_idx % 1000 == 0:\n",
    "        plot(frame_idx, all_rewards, losses)\n",
    "\n",
    "env.close()\n",
    "```\n",
    "\n",
    "- **num_frames**: 학습할 총 프레임 수입니다.\n",
    "- **batch_size**: 미니배치의 크기입니다.\n",
    "- **gamma**: 할인율입니다.\n",
    "- **losses**: 손실 값을 저장하는 리스트입니다.\n",
    "- **all_rewards**: 보상 값을 저장하는 리스트입니다.\n",
    "- **episode_reward**: 에피소드 당 보상 값을 저장하는 변수입니다.\n",
    "- **state = env.reset()**: 환경을 초기화하고 초기 상태를 가져옵니다.\n",
    "- **epsilon = epsilon_by_frame(frame_idx)**: 현재 프레임 인덱스에 따라 epsilon 값을 계산합니다.\n",
    "- **action = model.act(state, epsilon)**: 에이전트의 행동을 결정합니다.\n",
    "- **obs, reward, terminated, truncated, info = env.step(action)**: 환경에서 한 스텝을 진행하고 결과를 가져옵니다.\n",
    "- **replay_buffer.push(state, action, reward, obs, done)**: 경험을 리플레이 버퍼에 추가합니다.\n",
    "- **state = obs**: 다음 상태를 현재 상태로 업데이트합니다.\n",
    "- **episode_reward += reward**: 에피소드 보상에 현재 보상을 더합니다.\n",
    "- **if done**: 에피소드가 끝난 경우 환경을 초기화합니다.\n",
    "- **if len(replay_buffer) > replay_initial**: 리플레이 버퍼가 일정 크기 이상인 경우 학습을 진행합니다.\n",
    "- **if frame_idx % 1000 == 0**: 1000 프레임마다 학습 과정을 시각화합니다.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
